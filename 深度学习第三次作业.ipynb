{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7a54bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\HUAWEI\\AppData\\Local\\Temp\\jieba.cache\n",
      "DEBUG:jieba:Loading model from cache C:\\Users\\HUAWEI\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.668 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.668 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 作业三：从上面链接给定的语料库中均匀抽取200个段落（每个段落大于500个词），\n",
    "# 每个段落的标签就是对应段落所属的小说。利用LDA模型对于文本建模，并把每个段落表示为主题分布后进行分类。\n",
    "# 验证与分析分类结果，\n",
    "# （1）在不同数量的主题个数下分类性能的变化；\n",
    "# （2）以\"词\"和以\"字\"为基本单元下分类结果有什么差异？\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import jieba\n",
    "from sklearn.cluster import KMeans\n",
    "num_paragraphs = 200  # 抽取段落数\n",
    "min_length = 500  # 每个段落最小长度\n",
    "basepath = 'C:\\\\Users\\\\HUAWEI\\\\Desktop\\\\深度学习与自然语言处理\\\\第三次作业'  #文件夹位置\n",
    "with open(basepath+'\\\\jyxstxtqj_downcc.com\\\\inf.txt','r',encoding='ANSI') as f:\n",
    "    booklist = f.read().split(',') #读目录 \n",
    "with open(basepath+\"\\\\cn_stopwords.txt\",'r',encoding = 'utf-8-sig') as f:\n",
    "    stopwords = f.read().split() #读停用词\n",
    "stopwords2 = [' ','wwwcrcom','www','cr173','com','□','说道','说','道','便','笑','=','txt','\\n','本书来自www.cr173.com免费txt小说下载站','更多更新免费电子书请关注www.cr173.com','\\u3000','目录']\n",
    "stopwords.extend(stopwords2)\n",
    "content = []\n",
    "for book in booklist:\n",
    "    path = basepath+'\\\\jyxstxtqj_downcc.com\\\\'+ book +'.txt'\n",
    "    with open(path, 'r', encoding='ANSI') as f:\n",
    "        data_txt = f.read()  # 读取文本文件\n",
    "    words = jieba.lcut(data_txt)  # 结巴分词\n",
    "    words = [i for i in words if i not in stopwords]\n",
    "    pos = int(len(words)//13)\n",
    "    for i in range(13):\n",
    "        data_temp = []\n",
    "        data_temp = data_temp + words[i*pos:i*pos+500]\n",
    "        content.append(data_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d15259bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 208\n",
      "INFO:lda:vocab_size: 27954\n",
      "INFO:lda:n_words: 103931\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 1000\n",
      "INFO:lda:<0> log likelihood: -1365081\n",
      "INFO:lda:<10> log likelihood: -1118976\n",
      "INFO:lda:<20> log likelihood: -1099758\n",
      "INFO:lda:<30> log likelihood: -1091235\n",
      "INFO:lda:<40> log likelihood: -1085040\n",
      "INFO:lda:<50> log likelihood: -1079570\n",
      "INFO:lda:<60> log likelihood: -1076526\n",
      "INFO:lda:<70> log likelihood: -1073933\n",
      "INFO:lda:<80> log likelihood: -1071630\n",
      "INFO:lda:<90> log likelihood: -1069495\n",
      "INFO:lda:<100> log likelihood: -1067399\n",
      "INFO:lda:<110> log likelihood: -1065363\n",
      "INFO:lda:<120> log likelihood: -1064224\n",
      "INFO:lda:<130> log likelihood: -1063502\n",
      "INFO:lda:<140> log likelihood: -1062132\n",
      "INFO:lda:<150> log likelihood: -1060249\n",
      "INFO:lda:<160> log likelihood: -1058904\n",
      "INFO:lda:<170> log likelihood: -1058135\n",
      "INFO:lda:<180> log likelihood: -1057054\n",
      "INFO:lda:<190> log likelihood: -1056945\n",
      "INFO:lda:<200> log likelihood: -1054764\n",
      "INFO:lda:<210> log likelihood: -1054914\n",
      "INFO:lda:<220> log likelihood: -1054905\n",
      "INFO:lda:<230> log likelihood: -1053748\n",
      "INFO:lda:<240> log likelihood: -1052930\n",
      "INFO:lda:<250> log likelihood: -1052287\n",
      "INFO:lda:<260> log likelihood: -1052386\n",
      "INFO:lda:<270> log likelihood: -1052246\n",
      "INFO:lda:<280> log likelihood: -1052014\n",
      "INFO:lda:<290> log likelihood: -1051037\n",
      "INFO:lda:<300> log likelihood: -1050358\n",
      "INFO:lda:<310> log likelihood: -1050742\n",
      "INFO:lda:<320> log likelihood: -1050010\n",
      "INFO:lda:<330> log likelihood: -1050314\n",
      "INFO:lda:<340> log likelihood: -1049673\n",
      "INFO:lda:<350> log likelihood: -1049164\n",
      "INFO:lda:<360> log likelihood: -1049127\n",
      "INFO:lda:<370> log likelihood: -1049227\n",
      "INFO:lda:<380> log likelihood: -1048850\n",
      "INFO:lda:<390> log likelihood: -1049012\n",
      "INFO:lda:<400> log likelihood: -1048172\n",
      "INFO:lda:<410> log likelihood: -1048048\n",
      "INFO:lda:<420> log likelihood: -1048379\n",
      "INFO:lda:<430> log likelihood: -1047423\n",
      "INFO:lda:<440> log likelihood: -1047765\n",
      "INFO:lda:<450> log likelihood: -1047715\n",
      "INFO:lda:<460> log likelihood: -1048030\n",
      "INFO:lda:<470> log likelihood: -1047745\n",
      "INFO:lda:<480> log likelihood: -1047563\n",
      "INFO:lda:<490> log likelihood: -1047732\n",
      "INFO:lda:<500> log likelihood: -1047332\n",
      "INFO:lda:<510> log likelihood: -1046860\n",
      "INFO:lda:<520> log likelihood: -1047093\n",
      "INFO:lda:<530> log likelihood: -1045892\n",
      "INFO:lda:<540> log likelihood: -1046064\n",
      "INFO:lda:<550> log likelihood: -1046225\n",
      "INFO:lda:<560> log likelihood: -1046739\n",
      "INFO:lda:<570> log likelihood: -1046265\n",
      "INFO:lda:<580> log likelihood: -1045570\n",
      "INFO:lda:<590> log likelihood: -1045905\n",
      "INFO:lda:<600> log likelihood: -1046212\n",
      "INFO:lda:<610> log likelihood: -1045696\n",
      "INFO:lda:<620> log likelihood: -1045398\n",
      "INFO:lda:<630> log likelihood: -1045389\n",
      "INFO:lda:<640> log likelihood: -1045055\n",
      "INFO:lda:<650> log likelihood: -1044913\n",
      "INFO:lda:<660> log likelihood: -1045616\n",
      "INFO:lda:<670> log likelihood: -1045289\n",
      "INFO:lda:<680> log likelihood: -1045790\n",
      "INFO:lda:<690> log likelihood: -1045619\n",
      "INFO:lda:<700> log likelihood: -1044823\n",
      "INFO:lda:<710> log likelihood: -1045307\n",
      "INFO:lda:<720> log likelihood: -1045946\n",
      "INFO:lda:<730> log likelihood: -1045522\n",
      "INFO:lda:<740> log likelihood: -1044757\n",
      "INFO:lda:<750> log likelihood: -1045070\n",
      "INFO:lda:<760> log likelihood: -1044447\n",
      "INFO:lda:<770> log likelihood: -1044955\n",
      "INFO:lda:<780> log likelihood: -1044454\n",
      "INFO:lda:<790> log likelihood: -1044485\n",
      "INFO:lda:<800> log likelihood: -1044171\n",
      "INFO:lda:<810> log likelihood: -1044147\n",
      "INFO:lda:<820> log likelihood: -1044738\n",
      "INFO:lda:<830> log likelihood: -1044613\n",
      "INFO:lda:<840> log likelihood: -1044176\n",
      "INFO:lda:<850> log likelihood: -1044158\n",
      "INFO:lda:<860> log likelihood: -1044630\n",
      "INFO:lda:<870> log likelihood: -1044810\n",
      "INFO:lda:<880> log likelihood: -1044680\n",
      "INFO:lda:<890> log likelihood: -1043800\n",
      "INFO:lda:<900> log likelihood: -1044973\n",
      "INFO:lda:<910> log likelihood: -1044293\n",
      "INFO:lda:<920> log likelihood: -1044568\n",
      "INFO:lda:<930> log likelihood: -1045176\n",
      "INFO:lda:<940> log likelihood: -1044052\n",
      "INFO:lda:<950> log likelihood: -1044991\n",
      "INFO:lda:<960> log likelihood: -1044033\n",
      "INFO:lda:<970> log likelihood: -1044021\n",
      "INFO:lda:<980> log likelihood: -1043645\n",
      "INFO:lda:<990> log likelihood: -1044167\n",
      "INFO:lda:<999> log likelihood: -1044074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 韦小宝 陈家洛 萧中慧 康熙 卓天雄 周威信 张召重 皇帝 瞎子 公主\n",
      "Topic 1: 范蠡 剑士 剑 长剑 剑法 青衣 勾践 阿青 少女 风\n",
      "Topic 2: 一个 汉子 少年 中 老者 请 兵 这位 铁木真 罢\n",
      "Topic 3: 令狐冲 黄蓉 派 中 穆念慈 欧阳克 仪琳 弟子 丐帮 武功\n",
      "Topic 4: 中 皇帝 曰 中国 牋 百姓 杀 写 \t 小说\n",
      "Topic 5: 袁承志 胡斐 走 大汉 原来 青青 水笙 程灵素 见 知\n",
      "Topic 6: 事 没 师父 想 知道 心中 见 做 瞧 会\n",
      "Topic 7: 杀 听 中 想 姑娘 见到 一个 吃 张无忌 爷爷\n",
      "Topic 8: 中 一声 听 见 只见 两人 一个 突然 出 二人\n",
      "Topic 9: 麽 著 李文秀 後 於 曹云奇 苏普 甚 苏鲁克 爹爹\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "\n",
    "# 208个段落，每个段落用一个list表示\n",
    "paragraphs = content\n",
    "\n",
    "# 构建词袋模型\n",
    "vocab = set(word for paragraph in paragraphs for word in paragraph)\n",
    "word2id = dict((word, id) for id, word in enumerate(vocab))\n",
    "id2word = dict((id, word) for id, word in enumerate(vocab))\n",
    "doc_word_matrix = np.zeros((len(paragraphs), len(vocab)), dtype=np.int32)\n",
    "for i, paragraph in enumerate(paragraphs):\n",
    "    for word in paragraph:\n",
    "        doc_word_matrix[i, word2id[word]] += 1\n",
    "\n",
    "# 训练LDA模型\n",
    "model = lda.LDA(n_topics=10, n_iter=1000, random_state=1)\n",
    "model.fit(doc_word_matrix)\n",
    "\n",
    "# 输出每个主题的前10个关键词\n",
    "topic_word = model.topic_word_\n",
    "doc_topic = model.transform(doc_word_matrix)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(list(vocab))[np.argsort(topic_dist)][:-(10+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5900ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 8 8 8 8 8 8 8 8 8 8 8 8 \n",
      "\n",
      "9 0 0 0 0 3 0 0 8 3 3 9 9 \n",
      "\n",
      "3 0 8 0 11 0 0 0 0 0 6 0 15 \n",
      "\n",
      "11 13 13 13 11 2 0 0 3 3 4 13 13 \n",
      "\n",
      "1 15 15 15 15 2 15 2 15 1 2 2 6 \n",
      "\n",
      "9 9 6 1 9 1 9 1 1 9 9 1 9 \n",
      "\n",
      "6 6 6 7 7 7 14 7 7 14 5 14 6 \n",
      "\n",
      "1 7 3 0 4 6 7 11 6 13 7 7 3 \n",
      "\n",
      "15 15 15 2 15 2 2 15 15 2 2 2 2 \n",
      "\n",
      "6 12 7 12 11 7 12 6 14 7 14 1 5 \n",
      "\n",
      "6 4 3 12 4 12 4 14 14 4 12 11 14 \n",
      "\n",
      "6 14 7 7 4 7 7 14 14 14 7 4 5 \n",
      "\n",
      "6 11 11 8 8 11 8 8 8 8 8 11 11 \n",
      "\n",
      "1 6 13 13 12 12 12 5 6 12 7 6 7 \n",
      "\n",
      "15 2 2 2 15 2 2 2 2 2 15 2 15 \n",
      "\n",
      "10 10 10 10 10 10 10 10 10 4 10 10 4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = doc_topic\n",
    "# 聚类为16类\n",
    "kmeans = KMeans(n_clusters=16)\n",
    "kmeans.fit(data)\n",
    "# 输出每个样本所属的类别\n",
    "labels = kmeans.labels_\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],end = ' ')\n",
    "    if (i+1)%13 == 0:\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a82c23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.56614444e-04, 3.89425876e-05, 1.73770979e-01, ...,\n",
       "        2.99069098e-02, 3.93327489e-01, 2.39205257e-01],\n",
       "       [2.70477760e-05, 1.33645354e-02, 6.86221977e-02, ...,\n",
       "        1.48780946e-01, 2.47653716e-01, 4.27803685e-01],\n",
       "       [5.79204507e-05, 4.74453598e-04, 2.09535331e-05, ...,\n",
       "        1.54387987e-01, 2.51060327e-01, 4.33430149e-01],\n",
       "       ...,\n",
       "       [2.82304827e-05, 5.21755318e-01, 1.63566367e-05, ...,\n",
       "        2.05442863e-01, 1.86600587e-01, 3.67164855e-05],\n",
       "       [1.73661723e-05, 5.14646199e-01, 4.05896000e-05, ...,\n",
       "        2.29692832e-01, 1.42804422e-01, 7.54460207e-05],\n",
       "       [3.19739492e-05, 4.07046855e-01, 2.36167173e-02, ...,\n",
       "        2.46954337e-01, 1.93264347e-01, 7.84564963e-05]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda61f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "听: 0.0104\n",
      "中: 0.0064\n",
      "一个: 0.0059\n",
      "想: 0.0052\n",
      "爹爹: 0.0049\n",
      "Topic 2:\n",
      "韦小宝: 0.0086\n",
      "杨过: 0.0043\n",
      "弟子: 0.0041\n",
      "张无忌: 0.0040\n",
      "杀: 0.0038\n",
      "Topic 3:\n",
      "麽: 0.0094\n",
      "中: 0.0072\n",
      "见: 0.0064\n",
      "一个: 0.0056\n",
      "走: 0.0053\n",
      "Topic 4:\n",
      "见: 0.0066\n",
      "陈家洛: 0.0058\n",
      "武功: 0.0054\n",
      "听: 0.0032\n",
      "中: 0.0030\n",
      "Topic 5:\n",
      "范蠡: 0.0085\n",
      "中: 0.0073\n",
      "剑士: 0.0071\n",
      "袁承志: 0.0064\n",
      "长剑: 0.0042\n",
      "[[120   1 323  56   0]\n",
      " [166   0 325   0   9]\n",
      " [126   5 353  16   0]\n",
      " ...\n",
      " [ 60   0  81   0 359]\n",
      " [ 86   0  76   0 338]\n",
      " [ 44   0 131   0 256]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "doc_list=content\n",
    "K=5\n",
    "alpha=0.01\n",
    "beta=0.1\n",
    "num_iters=25\n",
    "# 将列表中的元素转换为集合\n",
    "word_dict = set(word for doc in doc_list for word in doc)\n",
    "# 将列表中的元素转换为字典\n",
    "word_dict = {word: i for i, word in enumerate(word_dict)}\n",
    "# 将列表中的元素转换为numpy数组\n",
    "doc_list = np.array([[word_dict[word] for word in doc] for doc in doc_list])\n",
    "M = len(doc_list)  # 文档数\n",
    "V = len(word_dict) # 词汇的大小\n",
    "z = np.random.randint(0, K, size=(M, len(doc_list[0])))  # 主题分布\n",
    "n_mz = np.zeros((M, K), dtype=int)  # 文档-主题分布\n",
    "n_zv = np.zeros((K, V), dtype=int)  # 主题-词分布\n",
    "n_z = np.zeros(K, dtype=int)  # 主题数量\n",
    "for i in range(M):\n",
    "    for j in range(len(doc_list[i])):\n",
    "        k = z[i][j]\n",
    "        n_mz[i][k] += 1\n",
    "        n_zv[k][doc_list[i][j]] += 1\n",
    "        n_z[k] += 1\n",
    "# Gibbs采样\n",
    "for iter in range(num_iters):\n",
    "    for i in range(M):\n",
    "        for j in range(len(doc_list[i])):\n",
    "            k = z[i][j]\n",
    "            n_mz[i][k] -= 1\n",
    "            n_zv[k][doc_list[i][j]] -= 1\n",
    "            n_z[k] -= 1\n",
    "            p = (n_mz[i] + alpha) * (n_zv[:, doc_list[i][j]] + beta) / (n_z + V * beta)\n",
    "            # 将代码向量化，以提高计算速度\n",
    "            k = np.random.choice(K, p=p / p.sum())\n",
    "            z[i][j] = k\n",
    "            n_mz[i][k] += 1\n",
    "            n_zv[k][doc_list[i][j]] += 1\n",
    "            n_z[k] += 1\n",
    "\n",
    "        \n",
    "# 输出主题-词分布\n",
    "for k in range(K):\n",
    "    print('Topic {}:'.format(k+1))\n",
    "    top5_positions = np.argsort(n_zv[k])[::-1][:5]\n",
    "    for v in top5_positions:\n",
    "        print('{}: {:.4f}'.format(list(word_dict.keys())[list(word_dict.values()).index(v)], n_zv[k][v] / n_z[k]))\n",
    "print(n_mz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bcc22df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 182  73  60   2   0   1  34   0 148]\n",
      "[  0 187   0  22   0  14  10 198   0  69]\n",
      "[  0 196   0   6   0   0   0 255   0  43]\n",
      "[  0 100   0   0   0   2   0 378   0  20]\n",
      "[  0 265  15  37   0   4   0 179   0   0]\n",
      "[  0 343   0  32   0   0   0  32   0  93]\n",
      "[  8 287   0  75   6   1   4  67   3  49]\n",
      "[  0 186   0  12   0   0   0 276   0  26]\n",
      "[  0  67   0   0  45  66   0 266   0  56]\n",
      "[  0  89   1  43   0   0   0 317   0  50]\n",
      "[  0 164   0  14   8   9   0 234   0  71]\n",
      "[  0 148   0  16   0   0   0 297   0  39]\n",
      "[  0 170   0  15   0  12   0 277   0  26]\n",
      "[  9   0 278   0   0   0 106   0 107   0]\n",
      "[  0   0   0 180 134  52   0   0  38  96]\n",
      "[  0  41 133 130   0   0   0  10  45 141]\n",
      "[  0   0   0   0  76 218   0   6  66 134]\n",
      "[  0  25   0   0 147 116   0  50  62 100]\n",
      "[  0  29   0   0  15 163   0  56 152  85]\n",
      "[  0   0   0   0  41  49  52  35 231  92]\n",
      "[  0   0   0  25  62 260  37  22  69  25]\n",
      "[  0   4  11 338   0 128   0   0  19   0]\n",
      "[ 29  39   0   0   0 158   0  72  92 110]\n",
      "[  0   0   5  50  18 144   0  73  54 156]\n",
      "[ 72   0   8   0   0   0   0   0 420   0]\n",
      "[  8   0  49   0   0   0  36   0 407   0]\n",
      "[  0   0  98 101  29 139   0  59   0  74]\n",
      "[  0   0   0  55  53 183   1   0   0 208]\n",
      "[  0  11  51   0  40  77   0 225   0  96]\n",
      "[  0 191   0  54   0 100   0  97   0  58]\n",
      "[  0   0   0 202 102  90   0   0   0 106]\n",
      "[  0  33   0  70   0  83   0   0   0 314]\n",
      "[ 22  10  17 306  26 119   0   0   0   0]\n",
      "[  0   2   0  98   2  73   0   0   0 325]\n",
      "[  0   0  10   4  29 369   0   0   0  88]\n",
      "[  0  44  25 181  24  78   0   0   0 148]\n",
      "[  0  19   0   0  80 154   0  61   0 186]\n",
      "[  0  37  48  65   0  71   0   0   7 272]\n",
      "[  1   0 149  36  36   0  47   2   5 224]\n",
      "[  0  35   0  32 268 108   1  24   0  32]\n",
      "[  2   0   0   0  91  44   0 201  48 114]\n",
      "[ 18  69   0   0   0  67   0 245  36  65]\n",
      "[ 36  11   0  20   0  27   4 347  55   0]\n",
      "[  0  19   0 140  49   0   0 206   0  86]\n",
      "[  0  32 185  34   0  69   5   6   0 169]\n",
      "[  0  16   6 164 147  13  13   0   0 141]\n",
      "[  0   0   0 240 119  31   2   0   0 108]\n",
      "[ 36   0   0 180   0 137   0  94   0  53]\n",
      "[ 21   5   0 245   2  65   2  78   0  82]\n",
      "[  0   0   0  15 228  41   0 102  62  52]\n",
      "[ 12   0   0   0   0  61   0 300   0 127]\n",
      "[  0  19   0  28   0  36   0 310  20  87]\n",
      "[393   1  35   0   0  24   0  47   0   0]\n",
      "[  0  92   0  16   0   0 253  47   3  89]\n",
      "[  0 133   0   3   0  46 159 111   5  43]\n",
      "[  0  25   0  44   0  45 279 107   0   0]\n",
      "[  0  24   0   0   3  47 272  52   4  98]\n",
      "[  0   0   0 120  17  62 239  19   0  43]\n",
      "[ 83  11   0   0   0   0 274 103   0  29]\n",
      "[  0  21   0   0   0  22 263 105   0  89]\n",
      "[  0  16   0   0   0   0 375   0   0 109]\n",
      "[ 19   0   0   0   0   0 284  81  41  75]\n",
      "[  0  46  13   0   5  54 265   0   2 115]\n",
      "[  0  79   0   0   0  37 230  10   0 144]\n",
      "[  0  12   0  14   1   0 348   0   0 125]\n",
      "[151   0   2   0  61   0   0   0 286   0]\n",
      "[ 79   0   0   0   0   0  88   3 330   0]\n",
      "[ 79  41   0 157  11  35   0  61  46  70]\n",
      "[162   0   0   0  16 179   0  15 120   8]\n",
      "[ 33   0   0   0  27   0   0  11 429   0]\n",
      "[ 74   1   0   0   0  10 100 216  99   0]\n",
      "[103   0   0  30   0  22  29  59 257   0]\n",
      "[ 38   0   0   0   0  52  23   0 310  77]\n",
      "[233   0  14   0   0   8  98  50  55  42]\n",
      "[ 79   0   0   0   0   0  91   9 321   0]\n",
      "[  0   9   0   0   0  34  55   0 399   3]\n",
      "[  0   0   0  14   0  41  37  96 292  20]\n",
      "[ 12   0   0   0   6  12  62   0 408   0]\n",
      "[202  37  77   1   0  41  50   0  13  79]\n",
      "[  0  16   0  26  28  36 284  21  13  76]\n",
      "[  0  80   0  10   0  29 325   0  56   0]\n",
      "[ 10   8   7  44   0 251  81  15  23  61]\n",
      "[  0  27   0  63   0  77   0  69 159 105]\n",
      "[  0   8   2  73   0 305   0   0   0 112]\n",
      "[ 35  39  16   0   0 302  22  67   0  19]\n",
      "[  0   8  84 208   5  74   0  19  21  81]\n",
      "[  0   0   0  22  38 221  25  11  40 143]\n",
      "[ 45   0  20 299   0   0   0   0  31 105]\n",
      "[  0 153   0  50   0  57   0  62  97  81]\n",
      "[  0  89  99 162   0 112   0   7   0  31]\n",
      "[  0  83  11  36   0  41 169  38  68  54]\n",
      "[425  28   1   0   0  16   0   0   0  30]\n",
      "[ 95  49  15  55   0  12  77   0   2 195]\n",
      "[60 91  0 71  0 77 69 49  0 83]\n",
      "[  0 132   0   0   0  54 132   0   0 182]\n",
      "[ 49  34   0  32 133  30  74   0   0 148]\n",
      "[ 19 177   0   3  14   0 140  76   5  66]\n",
      "[180  16   0  81   0  13   0  38   0 172]\n",
      "[  0  21   0  27 187  12  26  71   6 150]\n",
      "[ 15  74 136   0  18  73  90   0   0  94]\n",
      "[130   0   0  51   0   0   0 269   1  49]\n",
      "[ 20   0   0 277  12  91   0   0   3  97]\n",
      "[115  47   0 217   2   8   0  13  12  86]\n",
      "[158  79  29  28   6  37   0  11  10 142]\n",
      "[124  38   9 147   0  45   0  90  11  36]\n",
      "[ 33  78 255   0   0  22   0  10  17  85]\n",
      "[  6   2 278   0   0  79   0  59  10  66]\n",
      "[  0   0 241   0  96  40   0   0   0 123]\n",
      "[  0   7 325   0   0   2   0   9  58  99]\n",
      "[  0  90 313   0   0  14   0   0   0  83]\n",
      "[ 22  29 228  41   0  39   0  50   0  91]\n",
      "[  0   0 309  21  13  25   0  78   0  54]\n",
      "[ 39  21 267   0   0   0   0  88   0  85]\n",
      "[  0   2 269  28  37  28  30   7   0  99]\n",
      "[  1   0 164  74 152   0   0  10   0  99]\n",
      "[  0   0 178  70  79   0   0  11   0 162]\n",
      "[  0  64 144   0   0  45 127   8   0 112]\n",
      "[204   0  34   0 155  16   1  11   0  79]\n",
      "[  0   0   0   0   0  36   3  96 219 146]\n",
      "[  0  14  70  35  31  73 206   0   0  71]\n",
      "[  0   0 224   0   0  39   0 222  15   0]\n",
      "[  0  29   0   0  49  36  40  21   0 325]\n",
      "[  1  47   0   0  11 277   0  20   0 144]\n",
      "[182  10  24   0  31  60   0 101  28  64]\n",
      "[  1  52   0  59  25  15  59 182   0 107]\n",
      "[ 42 100  99  25   0 232   2   0   0   0]\n",
      "[  0   0   0  68   0 284   0  10   0 138]\n",
      "[287   8   0   0   0 106   0   8   0  91]\n",
      "[ 30   0 309   0   0   0   8  36   0 117]\n",
      "[ 10   0 225   0   2  12  11  51  94  95]\n",
      "[195  36  59  25  62  22   0  23   0  78]\n",
      "[  0  50   3 141  62 165   0   0   0  79]\n",
      "[  0  81   0  48  20  65   0  65 115 106]\n",
      "[  0  10   0 302   0  52   0 136   0   0]\n",
      "[ 33   0   0 223 170   0   0   0   2  72]\n",
      "[  0  42   0 287   0  46   0 104   0  21]\n",
      "[  6   0   0 171 166  32   0   0   0 125]\n",
      "[ 18   0 112  21 170  40   0   7  40  92]\n",
      "[ 29   0   0  11 209   0   0   3 133 115]\n",
      "[  0   0   0 141   0  87  38  28  85 121]\n",
      "[  0  73   0 201   0 121   0   0   0 105]\n",
      "[  0   7   0 228  78  65   0   0   0 122]\n",
      "[165   0   0  29  45   0   6   0 195  60]\n",
      "[ 23 207 187   6  10   0   4   0   0  63]\n",
      "[297  34   0   2   3  59   0   0   0 105]\n",
      "[238  23   0  46  79   0   0  10   0 104]\n",
      "[187   0   0   0 103  30   0   9   0 171]\n",
      "[ 37   0   0  13 335   0  18   0   0  97]\n",
      "[  0   0   0  31  23  65   0 301   0  80]\n",
      "[ 85  26  43   0  16   5   0  11   0 314]\n",
      "[ 77  72   3  54   0  59   0  12   5 218]\n",
      "[274   0   0  64   0   4   0   0   0 158]\n",
      "[ 86   0   0  51  27   0   0 245   0  91]\n",
      "[138  27   0  26   0  50   0  38   0 221]\n",
      "[  0  89   0  12 278  27   0   0   0  94]\n",
      "[208  18   0 148   0   0   0  59   0  67]\n",
      "[  9 167 168   3 133   2   0  18   0   0]\n",
      "[  0 115   0   0 250  36   0   0   0  99]\n",
      "[  0  33   0  42 315   0   0   0   0 110]\n",
      "[  0 288   0  46  80  64  16   0   6   0]\n",
      "[ 21 242   0   0  38  61   0  55   0  83]\n",
      "[  0  80   0   0 250  49   0  10   0 111]\n",
      "[  0 173   0   0  14  34   0 188   0  91]\n",
      "[  0 317   0   0  26  72   0  24   0  61]\n",
      "[  0 225   0   0  77  44   0  30   0 124]\n",
      "[  0 278   0  55   0  73   0  15   0  79]\n",
      "[  0 186   0   2 132  39   0  78   9  54]\n",
      "[  0  92   0  28 166  41   0   0   0 173]\n",
      "[  0 127  30  81  11  74   0  56   0 121]\n",
      "[380   0  32  28   0  44  16   0   0   0]\n",
      "[ 61  71   0  39  14 168   0  11   0 136]\n",
      "[  0  17   0   7   0  90   0  85 204  97]\n",
      "[ 30 245   0  22   0 126   0   0   1  76]\n",
      "[  0  35   0 184  15  14   9  60   0 183]\n",
      "[  4 114   0 213   0  46   0  32   0  91]\n",
      "[  1  83   0 293   0  60   0   0   0  63]\n",
      "[  0  68   0 149  27   0 132  14  21  89]\n",
      "[ 17   6   0 313  30   0  15  23  12  84]\n",
      "[  0  24   9 165  14  20   0  30   0 238]\n",
      "[  0   6   0  67 101   0  13  82   0 231]\n",
      "[  0  14   0  54  72   3 179  14   0 164]\n",
      "[  0   0   0   0 124  25  16  96   0 239]\n",
      "[  0 175  41  10   0 138   0  23   7 106]\n",
      "[  0 165  23   0   0 118   0   3   0 191]\n",
      "[  0  68   0   0   0 227  12   0   0 193]\n",
      "[  0  63   8  66   0 216   0   0   0 147]\n",
      "[  0 111   0   0   0 107   0  48   1 233]\n",
      "[  0  90   0   0  21  31   0  12   0 346]\n",
      "[  0 283  23   0   6  50   0   5   0 133]\n",
      "[  0 196   0  20  24  22   2   0   0 236]\n",
      "[  0 129   5   0  38  14   2   0   0 312]\n",
      "[ 13  93   0   7   0  21   0   0   0 366]\n",
      "[  0  77  57   0   0  85   0   0   0 281]\n",
      "[  0  99   3  15   9  37   0   0   0 337]\n",
      "[  0 128  16   0   0  31  63   0  13 249]\n",
      "[  0   0   0  18 234  42 179   0   0  27]\n",
      "[  0   0   0   0 235   6 249   0   0  10]\n",
      "[  0   0   0   0 300   0 146   0   0  54]\n",
      "[  0   0   0   0 387  45  68   0   0   0]\n",
      "[  0   0   0   0 450  50   0   0   0   0]\n",
      "[  0   0   0   0 359 141   0   0   0   0]\n",
      "[  0  29   0   0 355 105  11   0   0   0]\n",
      "[  0 102   0   0 309  84   5   0   0   0]\n",
      "[  0 208   0   0 166  82   5  13   0  26]\n",
      "[  0 349   0   0  76  19   0  56   0   0]\n",
      "[  0 260   0   0 171  16   0  53   0   0]\n",
      "[  0 291   0   0 169   0   0  40   0   0]\n",
      "[ 19 270   0  25  67  42   2   6   0   0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(n_mz)):\n",
    "    print(n_mz[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bccdab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 14 14 14 14 6 6 14 6 14 14 14 6 \n",
      "1 7 7 11 0 7 12 11 6 15 7 10 10 \n",
      "0 3 15 0 15 9 0 8 6 0 0 11 8 \n",
      "3 3 0 15 14 8 0 15 15 15 10 14 0 \n",
      "4 3 5 13 3 13 13 0 4 10 3 5 5 \n",
      "12 1 1 1 1 12 11 9 1 13 12 12 12 \n",
      "15 12 12 11 9 3 3 0 7 3 0 15 7 \n",
      "0 0 3 3 10 1 0 3 11 13 10 3 5 \n",
      "9 8 8 8 8 8 9 8 9 9 7 0 8 \n",
      "11 8 1 4 11 3 7 7 13 14 1 5 9 \n",
      "11 3 13 5 10 5 3 11 10 3 3 3 1 \n",
      "6 4 15 13 11 4 15 13 4 13 9 3 4 \n",
      "6 0 0 6 0 14 15 0 14 14 14 0 14 \n",
      "1 11 13 9 3 15 3 11 1 5 11 12 3 \n",
      "9 8 8 8 9 8 8 8 8 8 8 8 9 \n",
      "2 2 2 2 2 2 2 2 2 12 12 12 12 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "data = n_mz\n",
    "\n",
    "# 聚类为16类\n",
    "kmeans = KMeans(n_clusters=16)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# 输出每个样本所属的类别\n",
    "labels = kmeans.labels_\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],end = ' ')\n",
    "    if (i+1)%13 == 0:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff49a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9dc3810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import jieba\n",
    "from sklearn.cluster import KMeans\n",
    "num_paragraphs = 200  # 抽取段落数\n",
    "min_length = 500  # 每个段落最小长度\n",
    "basepath = 'C:\\\\Users\\\\HUAWEI\\\\Desktop\\\\深度学习与自然语言处理\\\\第三次作业'  #文件夹位置\n",
    "with open(basepath+'\\\\jyxstxtqj_downcc.com\\\\inf.txt','r',encoding='ANSI') as f:\n",
    "    booklist = f.read().split(',') #读目录 \n",
    "with open(basepath+\"\\\\cn_stopwords.txt\",'r',encoding = 'utf-8-sig') as f:\n",
    "    stopwords = f.read().split() #读停用词\n",
    "stopwords2 = [' ','wwwcrcom','www','w','c','r','cr173','com','□','说道','说','道','便','笑','=','txt','\\n','本书来自www.cr173.com免费txt小说下载站','更多更新免费电子书请关注www.cr173.com','\\u3000','目录']\n",
    "stopwords.extend(stopwords2)\n",
    "content = []\n",
    "for book in booklist:\n",
    "    path = basepath+'\\\\jyxstxtqj_downcc.com\\\\'+ book +'.txt'\n",
    "    with open(path, 'r', encoding='ANSI') as f:\n",
    "        data_txt = f.read()  # 读取文本文件\n",
    "    words = list(data_txt)  #按照字取\n",
    "    words = [i for i in words if i not in stopwords]\n",
    "    pos = int(len(words)//13)\n",
    "    for i in range(13):\n",
    "        data_temp = []\n",
    "        data_temp = data_temp + words[i*pos:i*pos+500]\n",
    "        content.append(data_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d350f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 208\n",
      "INFO:lda:vocab_size: 3318\n",
      "INFO:lda:n_words: 104000\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 1000\n",
      "INFO:lda:<0> log likelihood: -1039783\n",
      "INFO:lda:<10> log likelihood: -846716\n",
      "INFO:lda:<20> log likelihood: -814996\n",
      "INFO:lda:<30> log likelihood: -804420\n",
      "INFO:lda:<40> log likelihood: -797704\n",
      "INFO:lda:<50> log likelihood: -793247\n",
      "INFO:lda:<60> log likelihood: -791286\n",
      "INFO:lda:<70> log likelihood: -788452\n",
      "INFO:lda:<80> log likelihood: -786100\n",
      "INFO:lda:<90> log likelihood: -784662\n",
      "INFO:lda:<100> log likelihood: -783358\n",
      "INFO:lda:<110> log likelihood: -782162\n",
      "INFO:lda:<120> log likelihood: -781297\n",
      "INFO:lda:<130> log likelihood: -781063\n",
      "INFO:lda:<140> log likelihood: -779191\n",
      "INFO:lda:<150> log likelihood: -779499\n",
      "INFO:lda:<160> log likelihood: -778123\n",
      "INFO:lda:<170> log likelihood: -776691\n",
      "INFO:lda:<180> log likelihood: -775707\n",
      "INFO:lda:<190> log likelihood: -774777\n",
      "INFO:lda:<200> log likelihood: -773986\n",
      "INFO:lda:<210> log likelihood: -773999\n",
      "INFO:lda:<220> log likelihood: -773192\n",
      "INFO:lda:<230> log likelihood: -772036\n",
      "INFO:lda:<240> log likelihood: -772464\n",
      "INFO:lda:<250> log likelihood: -771877\n",
      "INFO:lda:<260> log likelihood: -771900\n",
      "INFO:lda:<270> log likelihood: -771314\n",
      "INFO:lda:<280> log likelihood: -770169\n",
      "INFO:lda:<290> log likelihood: -771128\n",
      "INFO:lda:<300> log likelihood: -771037\n",
      "INFO:lda:<310> log likelihood: -771049\n",
      "INFO:lda:<320> log likelihood: -770822\n",
      "INFO:lda:<330> log likelihood: -769237\n",
      "INFO:lda:<340> log likelihood: -768599\n",
      "INFO:lda:<350> log likelihood: -768430\n",
      "INFO:lda:<360> log likelihood: -768539\n",
      "INFO:lda:<370> log likelihood: -768145\n",
      "INFO:lda:<380> log likelihood: -768475\n",
      "INFO:lda:<390> log likelihood: -768858\n",
      "INFO:lda:<400> log likelihood: -767884\n",
      "INFO:lda:<410> log likelihood: -768368\n",
      "INFO:lda:<420> log likelihood: -767811\n",
      "INFO:lda:<430> log likelihood: -767288\n",
      "INFO:lda:<440> log likelihood: -768184\n",
      "INFO:lda:<450> log likelihood: -767584\n",
      "INFO:lda:<460> log likelihood: -767439\n",
      "INFO:lda:<470> log likelihood: -767167\n",
      "INFO:lda:<480> log likelihood: -766658\n",
      "INFO:lda:<490> log likelihood: -765950\n",
      "INFO:lda:<500> log likelihood: -765347\n",
      "INFO:lda:<510> log likelihood: -765409\n",
      "INFO:lda:<520> log likelihood: -764867\n",
      "INFO:lda:<530> log likelihood: -765660\n",
      "INFO:lda:<540> log likelihood: -765454\n",
      "INFO:lda:<550> log likelihood: -765329\n",
      "INFO:lda:<560> log likelihood: -765337\n",
      "INFO:lda:<570> log likelihood: -765378\n",
      "INFO:lda:<580> log likelihood: -766000\n",
      "INFO:lda:<590> log likelihood: -764634\n",
      "INFO:lda:<600> log likelihood: -764683\n",
      "INFO:lda:<610> log likelihood: -764783\n",
      "INFO:lda:<620> log likelihood: -765128\n",
      "INFO:lda:<630> log likelihood: -764869\n",
      "INFO:lda:<640> log likelihood: -764851\n",
      "INFO:lda:<650> log likelihood: -765306\n",
      "INFO:lda:<660> log likelihood: -765624\n",
      "INFO:lda:<670> log likelihood: -765435\n",
      "INFO:lda:<680> log likelihood: -766101\n",
      "INFO:lda:<690> log likelihood: -764859\n",
      "INFO:lda:<700> log likelihood: -765722\n",
      "INFO:lda:<710> log likelihood: -766606\n",
      "INFO:lda:<720> log likelihood: -766008\n",
      "INFO:lda:<730> log likelihood: -765497\n",
      "INFO:lda:<740> log likelihood: -765069\n",
      "INFO:lda:<750> log likelihood: -764694\n",
      "INFO:lda:<760> log likelihood: -765051\n",
      "INFO:lda:<770> log likelihood: -765457\n",
      "INFO:lda:<780> log likelihood: -765071\n",
      "INFO:lda:<790> log likelihood: -764912\n",
      "INFO:lda:<800> log likelihood: -765229\n",
      "INFO:lda:<810> log likelihood: -765134\n",
      "INFO:lda:<820> log likelihood: -765482\n",
      "INFO:lda:<830> log likelihood: -765069\n",
      "INFO:lda:<840> log likelihood: -765454\n",
      "INFO:lda:<850> log likelihood: -764916\n",
      "INFO:lda:<860> log likelihood: -764960\n",
      "INFO:lda:<870> log likelihood: -765724\n",
      "INFO:lda:<880> log likelihood: -765007\n",
      "INFO:lda:<890> log likelihood: -764934\n",
      "INFO:lda:<900> log likelihood: -765032\n",
      "INFO:lda:<910> log likelihood: -764568\n",
      "INFO:lda:<920> log likelihood: -765275\n",
      "INFO:lda:<930> log likelihood: -764779\n",
      "INFO:lda:<940> log likelihood: -764614\n",
      "INFO:lda:<950> log likelihood: -764930\n",
      "INFO:lda:<960> log likelihood: -765260\n",
      "INFO:lda:<970> log likelihood: -764696\n",
      "INFO:lda:<980> log likelihood: -765321\n",
      "INFO:lda:<990> log likelihood: -765141\n",
      "INFO:lda:<999> log likelihood: -764809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 子 老 心 见 想 死 里 白 没 什\n",
      "Topic 1: 手 中 出 身 时 声 两 心 然 三\n",
      "Topic 2: 杨 女 龙 家 陈 玉 洛 鱼 英 花\n",
      "Topic 3: 李 著 克 靖 苏 黄 郭 蓉 文 秀\n",
      "Topic 4: 马 子 声 铁 两 长 见 身 少 里\n",
      "Topic 5: 宝 韦 皇 公 兵 官 帝 中 宗 高\n",
      "Topic 6: 剑 士 青 长 吴 范 名 蠡 招 国\n",
      "Topic 7: 师 事 弟 山 兄 武 教 想 门 令\n",
      "Topic 8: 十 中 王 国 书 三 回 写 年 生\n",
      "Topic 9: 刀 中 著 子 萧 苗 袁 雄 夫 胡\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "\n",
    "# 208个段落，每个段落用一个list表示\n",
    "paragraphs = content\n",
    "\n",
    "# 构建词袋模型\n",
    "vocab = set(word for paragraph in paragraphs for word in paragraph)\n",
    "word2id = dict((word, id) for id, word in enumerate(vocab))\n",
    "id2word = dict((id, word) for id, word in enumerate(vocab))\n",
    "doc_word_matrix = np.zeros((len(paragraphs), len(vocab)), dtype=np.int32)\n",
    "for i, paragraph in enumerate(paragraphs):\n",
    "    for word in paragraph:\n",
    "        doc_word_matrix[i, word2id[word]] += 1\n",
    "\n",
    "# 训练LDA模型\n",
    "model = lda.LDA(n_topics=10, n_iter=1000, random_state=1)\n",
    "model.fit(doc_word_matrix)\n",
    "\n",
    "# 输出每个主题的前10个关键词\n",
    "topic_word = model.topic_word_\n",
    "doc_topic = model.transform(doc_word_matrix)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(list(vocab))[np.argsort(topic_dist)][:-(10+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d8d3b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 12 15 15 15 4 4 15 15 15 15 15 15 \n",
      "8 13 12 3 2 13 12 9 10 12 3 14 14 \n",
      "3 5 2 10 10 3 1 9 3 10 5 12 11 \n",
      "10 13 7 0 7 11 12 3 7 12 13 7 7 \n",
      "12 0 0 0 0 7 0 0 0 0 7 9 0 \n",
      "8 8 9 8 0 0 14 12 0 14 0 12 14 \n",
      "12 9 9 4 3 4 4 4 10 10 15 4 9 \n",
      "8 4 10 1 10 3 7 10 2 11 10 3 3 \n",
      "11 11 11 11 11 11 11 2 3 9 13 3 3 \n",
      "12 7 3 8 10 5 7 7 10 7 1 8 12 \n",
      "9 13 10 7 1 7 10 10 2 1 10 10 2 \n",
      "9 2 2 2 1 5 2 2 2 2 2 1 1 \n",
      "9 5 5 5 2 10 5 5 2 5 5 10 5 \n",
      "8 3 2 1 12 4 3 2 1 2 2 9 1 \n",
      "5 5 5 3 5 5 5 5 5 5 5 5 5 \n",
      "6 6 6 6 6 6 6 6 6 13 13 6 13 \n"
     ]
    }
   ],
   "source": [
    "data = doc_topic\n",
    "# 聚类为16类\n",
    "kmeans = KMeans(n_clusters=16)\n",
    "kmeans.fit(data)\n",
    "# 输出每个样本所属的类别\n",
    "labels = kmeans.labels_\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],end = ' ')\n",
    "    if (i+1)%13 == 0:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "548a1d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "手: 0.0271\n",
      "中: 0.0237\n",
      "刀: 0.0193\n",
      "身: 0.0141\n",
      "见: 0.0127\n",
      "Topic 2:\n",
      "主: 0.0179\n",
      "胡: 0.0166\n",
      "家: 0.0146\n",
      "子: 0.0142\n",
      "声: 0.0138\n",
      "Topic 3:\n",
      "师: 0.0409\n",
      "子: 0.0165\n",
      "令: 0.0143\n",
      "林: 0.0138\n",
      "夫: 0.0135\n",
      "Topic 4:\n",
      "宝: 0.0161\n",
      "子: 0.0144\n",
      "韦: 0.0139\n",
      "教: 0.0137\n",
      "天: 0.0137\n",
      "Topic 5:\n",
      "马: 0.0270\n",
      "女: 0.0208\n",
      "回: 0.0176\n",
      "十: 0.0173\n",
      "中: 0.0142\n",
      "Topic 6:\n",
      "剑: 0.0800\n",
      "士: 0.0254\n",
      "国: 0.0176\n",
      "青: 0.0173\n",
      "手: 0.0158\n",
      "Topic 7:\n",
      "子: 0.0274\n",
      "声: 0.0192\n",
      "汉: 0.0134\n",
      "姑: 0.0110\n",
      "见: 0.0109\n",
      "Topic 8:\n",
      "时: 0.0185\n",
      "天: 0.0149\n",
      "手: 0.0145\n",
      "石: 0.0144\n",
      "两: 0.0141\n",
      "Topic 9:\n",
      "中: 0.0151\n",
      "十: 0.0138\n",
      "年: 0.0134\n",
      "袁: 0.0115\n",
      "三: 0.0108\n",
      "Topic 10:\n",
      "李: 0.0178\n",
      "文: 0.0174\n",
      "里: 0.0143\n",
      "中: 0.0140\n",
      "出: 0.0133\n",
      "[[108   9   0 ...  26   0  37]\n",
      " [ 74  36   0 ...  53   0  95]\n",
      " [ 42  84   0 ...   7   0 305]\n",
      " ...\n",
      " [  0   0  34 ...  32   0  36]\n",
      " [ 15   0  35 ...  14   0   0]\n",
      " [  0  61   0 ...  18   3  55]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "doc_list=content\n",
    "K=10\n",
    "alpha=0.01\n",
    "beta=0.1\n",
    "num_iters=25\n",
    "# 将列表中的元素转换为集合\n",
    "word_dict = set(word for doc in doc_list for word in doc)\n",
    "# 将列表中的元素转换为字典\n",
    "word_dict = {word: i for i, word in enumerate(word_dict)}\n",
    "# 将列表中的元素转换为numpy数组\n",
    "doc_list = np.array([[word_dict[word] for word in doc] for doc in doc_list])\n",
    "M = len(doc_list)  # 文档数\n",
    "V = len(word_dict) # 词汇的大小\n",
    "z = np.random.randint(0, K, size=(M, len(doc_list[0])))  # 主题分布\n",
    "n_mz = np.zeros((M, K), dtype=int)  # 文档-主题分布\n",
    "n_zv = np.zeros((K, V), dtype=int)  # 主题-词分布\n",
    "n_z = np.zeros(K, dtype=int)  # 主题数量\n",
    "for i in range(M):\n",
    "    for j in range(len(doc_list[i])):\n",
    "        k = z[i][j]\n",
    "        n_mz[i][k] += 1\n",
    "        n_zv[k][doc_list[i][j]] += 1\n",
    "        n_z[k] += 1\n",
    "# Gibbs采样\n",
    "for iter in range(num_iters):\n",
    "    for i in range(M):\n",
    "        for j in range(len(doc_list[i])):\n",
    "            k = z[i][j]\n",
    "            n_mz[i][k] -= 1\n",
    "            n_zv[k][doc_list[i][j]] -= 1\n",
    "            n_z[k] -= 1\n",
    "            p = (n_mz[i] + alpha) * (n_zv[:, doc_list[i][j]] + beta) / (n_z + V * beta)\n",
    "            # 将代码向量化，以提高计算速度\n",
    "            k = np.random.choice(K, p=p / p.sum())\n",
    "            z[i][j] = k\n",
    "            n_mz[i][k] += 1\n",
    "            n_zv[k][doc_list[i][j]] += 1\n",
    "            n_z[k] += 1\n",
    "\n",
    "        \n",
    "# 输出主题-词分布\n",
    "for k in range(K):\n",
    "    print('Topic {}:'.format(k+1))\n",
    "    top5_positions = np.argsort(n_zv[k])[::-1][:5]\n",
    "    for v in top5_positions:\n",
    "        print('{}: {:.4f}'.format(list(word_dict.keys())[list(word_dict.values()).index(v)], n_zv[k][v] / n_z[k]))\n",
    "print(n_mz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57031a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 4 0 0 0 0 0 0 4 0 0 0 0 \n",
      "11 14 4 14 8 3 3 15 14 14 14 15 3 \n",
      "4 13 8 1 1 1 1 1 8 1 4 6 9 \n",
      "2 2 4 15 13 1 13 13 13 14 2 6 15 \n",
      "12 7 7 7 7 7 7 7 7 15 7 15 15 \n",
      "12 3 7 3 3 15 15 11 9 11 11 11 11 \n",
      "12 15 15 4 13 14 14 14 13 13 4 4 15 \n",
      "12 14 6 8 13 6 6 13 3 6 13 13 13 \n",
      "4 9 9 1 3 9 9 9 12 15 2 13 9 \n",
      "12 6 4 4 13 6 6 6 6 13 6 11 4 \n",
      "12 7 6 9 6 6 6 6 6 6 6 6 3 \n",
      "12 8 8 8 2 13 1 8 8 8 8 2 7 \n",
      "12 3 1 1 3 13 9 1 3 14 14 1 1 \n",
      "12 14 3 3 6 9 6 7 6 7 1 1 1 \n",
      "5 5 5 5 8 13 5 5 13 14 5 5 5 \n",
      "10 10 10 10 10 2 2 10 10 2 2 10 2 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "data = n_mz\n",
    "\n",
    "# 聚类为16类\n",
    "kmeans = KMeans(n_clusters=16)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# 输出每个样本所属的类别\n",
    "labels = kmeans.labels_\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i],end = ' ')\n",
    "    if (i+1)%13 == 0:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eacf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
